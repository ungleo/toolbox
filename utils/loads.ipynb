{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7824004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from project_secrets import REDSHIFT_CONNECTION_STRING, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fdb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_engine = create_engine(REDSHIFT_CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ded0d9",
   "metadata": {},
   "source": [
    "### Lake !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a064d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datalake\n",
    "\n",
    "collab_sessions_dl = \"\"\"\n",
    "\n",
    "unload($$\n",
    "\n",
    "select wt.session_id,\n",
    "  count(distinct wt.user_id) qq\n",
    "  from datalake.collab_time_v0_ext wt\n",
    "  group by 1\n",
    "  having qq > 1\n",
    "\n",
    "$$) to 's3://mural-data-lake-prod/data/processed/data-science/collab_sessions/collab_sessions'\n",
    "parquet\n",
    "iam_role 'arn:aws:iam::882722260434:role/redshiftSpectrum' allowoverwrite;\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e16f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_engine.execute(collab_sessions_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f971f00",
   "metadata": {},
   "source": [
    "### Warehouse !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warehouse\n",
    "\n",
    "collab_sessions_wh = \"\"\"\n",
    "\n",
    "drop table if exists datascience_stage.collab_sessions;\n",
    "create table datascience_stage.collab_sessions as\n",
    "\n",
    "    (select wt.session_id,\n",
    "        count(distinct wt.user_id) qq\n",
    "    from datalake.collab_time_v0_ext wt\n",
    "    group by 1\n",
    "    having qq > 1);\n",
    "\n",
    "grant select on datascience_stage.collab_sessions to periscope;\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521fd7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_engine.execute(collab_sessions_wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e3f4f9",
   "metadata": {},
   "source": [
    "## create from redshift to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_table_s3 = \"\"\"\n",
    "CREATE TABLE sarasa\n",
    "WITH (\n",
    "      external_location = 's3://my_athena_results/my_orc_stas_table/',\n",
    "      format = 'PARQUET')\n",
    "AS SELECT * \n",
    "FROM old_table;\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_engine.execute(query_table_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2535f8",
   "metadata": {},
   "source": [
    "## read from redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test = \"\"\"\n",
    "select top 50\n",
    "* \n",
    "from public.memberv2 \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_sql_query(query_test, red_engine)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c42a7",
   "metadata": {},
   "source": [
    "# read files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79942d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD parquet\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(key=AWS_ACCESS_KEY_ID, secret=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "path = 's3://mural-data-lake-prod/data/processed/data-science/hot_spots_from_bad_fps_v0/added_connector_count_14days_v0'\n",
    "df_tmp = pq.ParquetDataset(path, filesystem=fs)\n",
    "df_tmp = df_tmp.read_pandas()\n",
    "df_tmp = df_tmp.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV\n",
    "# levantar varios files de una misma ruta \n",
    "# !pip3 install \"dask[dataframe]\"\n",
    "import dask.dataframe as dd\n",
    "\n",
    "path = \"s3://mural-data-lake-prod/data/processed/data-science/temp/*.csv\"\n",
    "df_from_cvs = dd.read_csv(path,\n",
    "                          sep=\",\", \n",
    "                          storage_options={\"key\":AWS_ACCESS_KEY_ID, \n",
    "                                           \"secret\":AWS_SECRET_ACCESS_KEY}) \n",
    "df_from_cvs = df_from_cvs.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a16f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_cvs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ab3a4",
   "metadata": {},
   "source": [
    "# write files on S3\n",
    "### Save file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3', \n",
    "                  aws_access_key_id=AWS_ACCESS_KEY_ID, \n",
    "                  aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "\n",
    "csv_file_name = \"csv_file_name.csv\"\n",
    "bucket_name = 'mural-data-lake-prod'\n",
    "path_name = 'data/processed/data-science/temp/tmp.csv'\n",
    "\n",
    "df_to_upload.to_csv(csv_file_name)\n",
    "\n",
    "files=['interes_item_df']\n",
    "for i in files:\n",
    "    s3.upload_file(csv_file_name,bucket_name,path_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4121d12d",
   "metadata": {},
   "source": [
    "### Save file parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447687bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(key=AWS_ACCESS_KEY_ID, secret=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "filename= \"s3://mural-data-lake-prod/data/processed/data-science/temp/file_tmp.parquet\"\n",
    "df_to_upload.to_parquet(fs.open(filename,'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
